\relax 
\bibstyle{abbrvnat}
\citation{PaulRD,ganite}
\citation{PaulRD,PaulR,Sheng}
\citation{Johansson,UriSha,SITE,Negar_2}
\citation{CEVAE,TEDEV,vowels2021targeted}
\citation{bonheme2023the}
\citation{Khan2024OnTE}
\citation{louizos2018learning}
\citation{JimenezRezende2018TamingV,Boom2020DynamicNO}
\citation{Gunn,Hill,Khan2024OnTE}
\@LN@col{1}
\@LN{0}{0}
\@LN{1}{0}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{}\protected@file@percent }
\@LN{2}{0}
\@LN{3}{0}
\@LN{4}{0}
\@LN{5}{0}
\@LN{6}{0}
\@LN{7}{0}
\@LN{8}{0}
\@LN{9}{0}
\@LN{10}{0}
\@LN{11}{0}
\@LN{12}{0}
\@LN{13}{0}
\@LN{14}{0}
\@LN{15}{0}
\@LN{16}{0}
\@LN{17}{0}
\@LN{18}{0}
\@LN{19}{0}
\@LN{20}{0}
\@LN{21}{0}
\@LN{22}{0}
\@LN@col{2}
\@LN{23}{0}
\@LN{24}{0}
\@LN{25}{0}
\@LN{26}{0}
\@LN{27}{0}
\@LN{28}{0}
\@LN{29}{0}
\@LN{30}{0}
\@LN{31}{0}
\@LN{32}{0}
\@LN{33}{0}
\@LN{34}{0}
\@LN{35}{0}
\@LN{36}{0}
\@LN{37}{0}
\@LN{38}{0}
\@LN{39}{0}
\@LN{40}{0}
\@LN{41}{0}
\@LN{42}{0}
\@LN{43}{0}
\@LN{44}{0}
\@LN{45}{0}
\@LN{46}{0}
\@LN{47}{0}
\@LN{48}{0}
\@LN{49}{0}
\@LN{50}{0}
\@LN{51}{0}
\@LN{52}{0}
\@LN{53}{0}
\@LN{54}{0}
\@LN{55}{0}
\@LN{56}{0}
\@LN{57}{0}
\@LN{58}{0}
\@LN{59}{0}
\@LN{60}{0}
\@LN{61}{0}
\@LN{62}{0}
\@LN{63}{0}
\@LN{64}{0}
\@LN{65}{0}
\@LN{66}{0}
\@LN{67}{0}
\@LN{68}{0}
\@LN{69}{0}
\citation{PaulRD}
\citation{PaulRD,PaulR,Sheng}
\citation{conf}
\citation{Johansson,UriSha,SITE,Negar_2}
\citation{Decomp,Negar,kuang2020data,Anpeng,Ortho}
\citation{CEVAE}
\citation{TEDEV}
\citation{Khan2024OnTE}
\citation{vowels2021targeted,Khan2024OnTE}
\citation{Mai,doersch2021tutorialvariationalautoencoders,FONDUE}
\citation{Levina2004MaximumLE,Facco2017EstimatingTI,Gong2018OnTI,Ansuini2019IntrinsicDO,pope2021intrinsic}
\citation{Boom2020DynamicNO}
\citation{Negar,Khan2024OnTE}
\citation{PaulRD,assumptions,Imbens}
\citation{Rubin}
\citation{Decomp,Khan2024OnTE}
\citation{VAE}
\@LN@col{1}
\@LN{70}{1}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{}\protected@file@percent }
\newlabel{sec:related-work}{{2}{2}}
\@LN{71}{1}
\@LN{72}{1}
\@LN{73}{1}
\@LN{74}{1}
\@LN{75}{1}
\@LN{76}{1}
\@LN{77}{1}
\@LN{78}{1}
\@LN{79}{1}
\@LN{80}{1}
\@LN{81}{1}
\@LN{82}{1}
\@LN{83}{1}
\@LN{84}{1}
\@LN{85}{1}
\@LN{86}{1}
\@LN{87}{1}
\@LN{88}{1}
\@LN{89}{1}
\@LN{90}{1}
\@LN{91}{1}
\@LN{92}{1}
\@LN{93}{1}
\@LN{94}{1}
\@LN{95}{1}
\@LN{96}{1}
\@LN{97}{1}
\@LN{98}{1}
\@LN{99}{1}
\@LN{100}{1}
\@LN{101}{1}
\@LN{102}{1}
\@LN{103}{1}
\@LN{104}{1}
\@LN{105}{1}
\@LN{106}{1}
\@LN{107}{1}
\@LN{108}{1}
\@LN{109}{1}
\@LN{110}{1}
\@LN{111}{1}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background Knowledge}{2}{}\protected@file@percent }
\newlabel{sec:background}{{3}{2}}
\@LN{112}{1}
\@LN{113}{1}
\@LN{114}{1}
\@LN{115}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Treatment Effect Estimation}{2}{}\protected@file@percent }
\newlabel{sec:background-tee}{{3.1}{2}}
\@LN{116}{1}
\@LN{117}{1}
\@LN{118}{1}
\@LN{119}{1}
\@LN{120}{1}
\@LN{121}{1}
\@LN{122}{1}
\@LN{123}{1}
\@LN{124}{1}
\@LN{125}{1}
\@LN@col{2}
\@LN{126}{1}
\@LN{127}{1}
\@LN{128}{1}
\@LN{129}{1}
\@LN{130}{1}
\@LN{131}{1}
\@LN{132}{1}
\@LN{133}{1}
\@LN{134}{1}
\@LN{135}{1}
\@LN{136}{1}
\@LN{137}{1}
\@LN{138}{1}
\@LN{139}{1}
\@LN{140}{1}
\@LN{141}{1}
\@LN{142}{1}
\@LN{143}{1}
\@LN{144}{1}
\@LN{145}{1}
\@LN{146}{1}
\@LN{147}{1}
\@LN{148}{1}
\@LN{149}{1}
\@LN{150}{1}
\@LN{151}{1}
\@LN{152}{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Variational Autoencoders}{2}{}\protected@file@percent }
\newlabel{sec:background-vae}{{3.2}{2}}
\newlabel{eq:elbo}{{2}{2}}
\@LN{153}{1}
\@LN{154}{1}
\@LN{155}{1}
\citation{louizos2018learning}
\citation{maddison2017the}
\citation{JimenezRezende2018TamingV}
\@LN@col{1}
\@LN{156}{2}
\@writefile{toc}{\contentsline {section}{\numberline {4}Method}{3}{}\protected@file@percent }
\newlabel{sec:method}{{4}{3}}
\@LN{157}{2}
\@LN{158}{2}
\@LN{159}{2}
\@LN{160}{2}
\@LN{161}{2}
\@LN{162}{2}
\@LN{163}{2}
\@LN{164}{2}
\@LN{165}{2}
\@LN{166}{2}
\@LN{167}{2}
\@LN{168}{2}
\@LN{169}{2}
\@LN{170}{2}
\@LN{171}{2}
\@LN{172}{2}
\@LN{173}{2}
\@LN{174}{2}
\@LN{175}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture diagram of the GLOVE-ITE (GECO and $L_0$ Optimization for Variational Estimation of Individual Treatment Effects) model: Four embedding spaces are learnt from a shared embedding, each processed through separate masks ($\|\mathbf  {m}_{\Gamma }\|_0,\|\mathbf  {m}_{\Delta }\|_0,\|\mathbf  {m}_{\Upsilon }\|_0,\|\mathbf  {m}_{\Omega }\|_0$). These latent spaces serve different roles: $\Gamma $ and $\Delta $ are leveraged for treatment prediction ($\qopname  \relax o{log}p( t | \mathbf  {z}_{\Gamma ,\Delta }^\prime )$), $\Delta $ and $\Upsilon $ facilitate outcome prediction and constraint enforcement ($C( \hat  {y}_{\Delta , \Upsilon }^\prime , t ) $), $\Upsilon $ is used for discrepancy loss ($\mathcal  {L}_\mathit  {disc}$), $\Omega $ is being used to learn irrelevant variables and all four embeddings jointly contribute to optimizing the reconstruction objective ($\qopname  \relax o{log}p( \mathbf  {x} | \mathbf  {z}_{\Gamma , \Delta , \Upsilon , \Omega }^\prime )$).}}{3}{}\protected@file@percent }
\newlabel{fig:arch}{{1}{3}}
\@LN{176}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}$L_0$ sparsity objective}{3}{}\protected@file@percent }
\newlabel{sec:l0}{{4.1}{3}}
\@LN@col{2}
\@LN{177}{2}
\@LN{178}{2}
\@LN{179}{2}
\@LN{180}{2}
\@LN{181}{2}
\@LN{182}{2}
\@LN{183}{2}
\@LN{184}{2}
\@LN{185}{2}
\@LN{186}{2}
\@LN{187}{2}
\@LN{188}{2}
\@LN{189}{2}
\@LN{190}{2}
\@LN{191}{2}
\@LN{192}{2}
\@LN{193}{2}
\@LN{194}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Prioritizing prediction performance}{3}{}\protected@file@percent }
\newlabel{sec:geco}{{4.2}{3}}
\newlabel{eq:elbo-mod}{{6}{3}}
\citation{Khan2024OnTE}
\citation{Gunn}
\citation{Hill}
\citation{Khan2024OnTE}
\citation{Negar,Khan2024OnTE}
\citation{UriSha}
\@LN@col{1}
\@LN{195}{3}
\@LN{196}{3}
\@LN{197}{3}
\@LN{198}{3}
\@LN{199}{3}
\@LN{200}{3}
\@LN{201}{3}
\@LN{202}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Implementation Details}{4}{}\protected@file@percent }
\newlabel{sec:details}{{4.3}{4}}
\newlabel{eq:overall}{{8}{4}}
\newlabel{eq:elbo-mod}{{9}{4}}
\@LN{203}{3}
\@LN{204}{3}
\@LN{205}{3}
\@LN{206}{3}
\@LN{207}{3}
\@LN{208}{3}
\@LN{209}{3}
\@LN{210}{3}
\@LN{211}{3}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{4}{}\protected@file@percent }
\@LN{212}{3}
\@LN{213}{3}
\@LN{214}{3}
\@LN{215}{3}
\@LN{216}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Evaluation criteria}{4}{}\protected@file@percent }
\@LN{217}{3}
\@LN{218}{3}
\@LN@col{2}
\@LN{219}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Datasets}{4}{}\protected@file@percent }
\@LN{220}{3}
\@LN{221}{3}
\@LN{222}{3}
\@LN{223}{3}
\@LN{224}{3}
\@LN{225}{3}
\@LN{226}{3}
\@LN{227}{3}
\@LN{228}{3}
\@LN{229}{3}
\@LN{230}{3}
\@LN{231}{3}
\@LN{232}{3}
\@LN{233}{3}
\@LN{234}{3}
\@LN{235}{3}
\@LN{236}{3}
\@LN{237}{3}
\@LN{238}{3}
\@LN{239}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Experiment details}{4}{}\protected@file@percent }
\@LN{240}{3}
\@LN{241}{3}
\@LN{242}{3}
\@LN{243}{3}
\@LN{244}{3}
\@LN{245}{3}
\@LN{246}{3}
\@LN{247}{3}
\@LN{248}{3}
\@LN{249}{3}
\@LN{250}{3}
\@LN{251}{3}
\@LN{252}{3}
\@LN{253}{3}
\@LN{254}{3}
\@LN{255}{3}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Results}{4}{}\protected@file@percent }
\@LN{256}{3}
\@LN{257}{3}
\@LN{258}{3}
\@LN{259}{3}
\@LN{260}{3}
\@LN{261}{3}
\@LN{262}{3}
\@LN{263}{3}
\@LN{264}{3}
\@LN{265}{3}
\@LN{266}{3}
\@LN{267}{3}
\@LN{268}{3}
\@LN{269}{3}
\@LN{270}{3}
\@LN{271}{3}
\@LN{272}{3}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces PEHE (std) results (lower is better) on the IHDP dataset (first 30 realizations), evaluated across varying number of irrelevant variables ($\#\Omega $) and the active latent dimensionality used by each encoder during learning.}}{5}{}\protected@file@percent }
\newlabel{tab:IHDP}{{1}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces PEHE (std) results (lower is better) on the synthetic dataset (8×8×8×$\Omega $), evaluated across varying number of irrelevant variables ($\#\Omega $) and the active latent dimensionality used by each encoder during learning.}}{5}{}\protected@file@percent }
\newlabel{tab:synthetic}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of three key training metrics: Mean Squared Error Constrained (MSEC:can be negative) relative to the threshold $\tau $, the Lagrange multiplier ($\lambda $), and the number of active latent dimensions on synthetic data with an 8×8×8 dimensional structure. Dimmed lines represent fixed $\lambda $, while brighter lines indicate learnable $\lambda $ (best viewed in color).}}{5}{}\protected@file@percent }
\newlabel{fig:learning_curves}{{2}{5}}
\@LN@col{1}
\@LN{273}{4}
\@LN{274}{4}
\@LN{275}{4}
\@LN{276}{4}
\@LN{277}{4}
\@LN{278}{4}
\@LN{279}{4}
\@LN{280}{4}
\@LN@col{2}
\@LN{281}{4}
\@LN{282}{4}
\@LN{283}{4}
\@LN{284}{4}
\@LN{285}{4}
\@LN{286}{4}
\@LN{287}{4}
\@LN{288}{4}
\citation{Fisher,Khan2024OnTE}
\citation{CEVAE,lowe2022amortized}
\citation{vowels2021targeted}
\bibdata{mybibfile}
\bibcite{Ansuini2019IntrinsicDO}{{1}{2019}{{Ansuini et~al.}}{{Ansuini, Laio, Macke, and Zoccolan}}}
\bibcite{FONDUE}{{2}{2022}{{Bonheme and Grzes}}{{}}}
\bibcite{bonheme2023the}{{3}{2023}{{Bonheme and Grzes}}{{}}}
\bibcite{Boom2020DynamicNO}{{4}{2020}{{Boom et~al.}}{{Boom, Wauthier, Verbelen, and Dhoedt}}}
\bibcite{Gunn}{{5}{1992}{{Brooks-Gunn et~al.}}{{Brooks-Gunn, ruey Liaw, and Klebanov}}}
\@LN@col{1}
\@LN{289}{5}
\@LN{290}{5}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results of the ablation study on the loss function, evaluated on the IHDP dataset with five irrelevant variables. Bold values indicate the best performance (lowest error).}}{6}{}\protected@file@percent }
\newlabel{tab:ablation}{{3}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Compression analysis of the proposed method on synthetic data with an 8×8×8×5 dimensional structure, evaluated under varying initial dimensionality settings. }}{6}{}\protected@file@percent }
\newlabel{tab:compression}{{4}{6}}
\@LN{291}{5}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Qualitative Evaluation of Model Performance}{6}{}\protected@file@percent }
\@LN{292}{5}
\@LN{293}{5}
\@LN{294}{5}
\@LN{295}{5}
\@LN{296}{5}
\@LN{297}{5}
\@LN{298}{5}
\@LN{299}{5}
\@LN{300}{5}
\@LN{301}{5}
\@LN{302}{5}
\@LN{303}{5}
\@LN{304}{5}
\@LN{305}{5}
\@LN{306}{5}
\@LN{307}{5}
\@LN{308}{5}
\@LN{309}{5}
\@LN{310}{5}
\@LN{311}{5}
\@LN{312}{5}
\@LN{313}{5}
\@LN{314}{5}
\@LN{315}{5}
\@LN{316}{5}
\@LN{317}{5}
\@LN{318}{5}
\@LN{319}{5}
\@LN{320}{5}
\@LN{321}{5}
\@LN{322}{5}
\@LN{323}{5}
\@LN{324}{5}
\@LN{325}{5}
\@LN{326}{5}
\@LN@col{2}
\@LN{327}{5}
\@LN{328}{5}
\@LN{329}{5}
\@LN{330}{5}
\@LN{331}{5}
\@LN{332}{5}
\@LN{333}{5}
\@LN{334}{5}
\@LN{335}{5}
\@LN{336}{5}
\@LN{337}{5}
\@LN{338}{5}
\@LN{339}{5}
\@LN{340}{5}
\@LN{341}{5}
\@LN{342}{5}
\@LN{343}{5}
\@LN{344}{5}
\@LN{345}{5}
\@LN{346}{5}
\@LN{347}{5}
\@LN{348}{5}
\@LN{349}{5}
\@LN{350}{5}
\@LN{351}{5}
\@LN{352}{5}
\@LN{353}{5}
\@LN{354}{5}
\@LN{355}{5}
\@LN{356}{5}
\@LN{357}{5}
\@LN{358}{5}
\@LN{359}{5}
\@LN{360}{5}
\@LN{361}{5}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{6}{}\protected@file@percent }
\@LN{362}{5}
\@LN{363}{5}
\@LN{364}{5}
\@LN{365}{5}
\@LN{366}{5}
\@LN{367}{5}
\@LN{368}{5}
\@LN{369}{5}
\@LN{370}{5}
\@LN{371}{5}
\@LN{372}{5}
\@LN{373}{5}
\@LN{374}{5}
\@LN{375}{5}
\@LN{376}{5}
\@LN{377}{5}
\@LN{378}{5}
\@LN{379}{5}
\@LN{380}{5}
\@LN{381}{5}
\@LN{382}{5}
\@LN{383}{5}
\@LN{384}{5}
\@LN{385}{5}
\bibcite{Ortho}{{6}{2022}{{Cheng et~al.}}{{Cheng, Liao, Liu, Ma, Xu, and Zheng}}}
\bibcite{doersch2021tutorialvariationalautoencoders}{{7}{2021}{{Doersch}}{{}}}
\bibcite{Facco2017EstimatingTI}{{8}{2017}{{Facco et~al.}}{{Facco, d’Errico, Rodriguez, and Laio}}}
\bibcite{Fisher}{{9}{2018}{{Fisher et~al.}}{{Fisher, Rudin, and Dominici}}}
\bibcite{Gong2018OnTI}{{10}{2018}{{Gong et~al.}}{{Gong, Boddeti, and Jain}}}
\bibcite{Negar}{{11}{2019{}}{{Hassanpour and Greiner}}{{}}}
\bibcite{Negar_2}{{12}{2019{}}{{Hassanpour and Greiner}}{{}}}
\bibcite{Hill}{{13}{2011}{{Hill}}{{}}}
\bibcite{Imbens}{{14}{2015}{{Imbens and Rubin}}{{}}}
\bibcite{Johansson}{{15}{2016}{{Johansson et~al.}}{{Johansson, Shalit, and Sontag}}}
\bibcite{Khan2024OnTE}{{16}{2024}{{Khan et~al.}}{{Khan, Schaffernicht, and Stork}}}
\bibcite{VAE}{{17}{2014}{{Kingma and Welling}}{{}}}
\bibcite{Decomp}{{18}{2017}{{Kuang et~al.}}{{Kuang, Cui, Li, Jiang, Yang, and Wang}}}
\bibcite{conf}{{19}{2019}{{Kuang et~al.}}{{Kuang, Cui, Li, Jiang, Wang, Wang, and Yang}}}
\bibcite{kuang2020data}{{20}{2020}{{Kuang et~al.}}{{Kuang, Cui, Zou, Li, Tao, Wu, and Yang}}}
\bibcite{Levina2004MaximumLE}{{21}{2004}{{Levina and Bickel}}{{}}}
\bibcite{Sheng}{{22}{2016}{{Li et~al.}}{{Li, Vlassis, Kawale, and Fu}}}
\bibcite{CEVAE}{{23}{2017}{{Louizos et~al.}}{{Louizos, Shalit, Mooij, Sontag, Zemel, and Welling}}}
\bibcite{louizos2018learning}{{24}{2018}{{Louizos et~al.}}{{Louizos, Welling, and Kingma}}}
\bibcite{lowe2022amortized}{{25}{2022}{{L{\"o}we et~al.}}{{L{\"o}we, Madras, Zemel, and Welling}}}
\bibcite{maddison2017the}{{26}{2017}{{Maddison et~al.}}{{Maddison, Mnih, and Teh}}}
\bibcite{Mai}{{27}{2020}{{Mai~Ngoc and Hwang}}{{}}}
\bibcite{pope2021intrinsic}{{28}{2021}{{Pope et~al.}}{{Pope, Zhu, Abdelkader, Goldblum, and Goldstein}}}
\bibcite{JimenezRezende2018TamingV}{{29}{2018}{{Rezende and Viola}}{{}}}
\bibcite{PaulR}{{30}{1987}{{Rosenbaum}}{{}}}
\bibcite{PaulRD}{{31}{1983}{{Rosenbaum and Rubin.}}{{}}}
\bibcite{Rubin}{{32}{2005{}}{{Rubin}}{{}}}
\bibcite{assumptions}{{33}{2005{}}{{Rubin}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Feature permutation analysis demonstrating the inference and disentanglement of latent across BCE, MSE, and PEHE tasks.}}{7}{}\protected@file@percent }
\newlabel{fig:permutation}{{3}{7}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Four learned masks showing active (green) and inactive (white) latent dimensions. Each mask activates distinct subsets of latent dimensions with minimal overlap—only one dimension is shared across all masks (visible as green in a single column)—indicating that the masks are learning largely independently. The observed overlap is just 2.5\%, highlighting the natural disentanglement effect of integrating sparsity-inducing masks within the VAE framework.}}{7}{}\protected@file@percent }
\newlabel{fig:ortho}{{4}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualization of active dimensionality and PEHE as functions of MSE tolerance.}}{7}{}\protected@file@percent }
\newlabel{fig:tolerance}{{5}{7}}
\@LN{386}{6}
\@LN{387}{6}
\@LN{388}{6}
\@LN{389}{6}
\@LN{390}{6}
\@LN{391}{6}
\@LN{392}{6}
\@LN{393}{6}
\@LN{394}{6}
\@LN{395}{6}
\@LN{396}{6}
\@LN{397}{6}
\@LN{398}{6}
\@LN{399}{6}
\@LN{400}{6}
\@LN{401}{6}
\@LN{402}{6}
\@LN{403}{6}
\@LN{404}{6}
\@LN{405}{6}
\@LN{406}{6}
\@LN{407}{6}
\@LN@col{2}
\@LN{408}{6}
\@LN{409}{6}
\@LN{410}{6}
\@LN{411}{6}
\@LN{412}{6}
\@LN{413}{6}
\@LN{414}{6}
\@LN{415}{6}
\@LN{416}{6}
\@LN{417}{6}
\@LN{418}{6}
\@LN{419}{6}
\@LN{420}{6}
\@LN{421}{6}
\@LN{422}{6}
\@LN{423}{6}
\@LN{424}{6}
\@LN{425}{6}
\@LN{426}{6}
\@LN{427}{6}
\@LN{428}{6}
\@LN{429}{6}
\@LN{430}{6}
\@LN{431}{6}
\@LN{432}{6}
\@LN{433}{6}
\@LN{434}{6}
\@LN{435}{6}
\@LN{436}{6}
\@LN{437}{6}
\@LN{438}{6}
\@LN{439}{6}
\@LN{440}{6}
\@LN{441}{6}
\@LN{442}{6}
\@LN{443}{6}
\@LN{444}{6}
\@LN{445}{6}
\@LN{446}{6}
\@LN{447}{6}
\@LN{448}{6}
\@LN{449}{6}
\@LN{450}{6}
\@LN{451}{6}
\@LN{452}{6}
\@LN{453}{6}
\@LN{454}{6}
\@LN{455}{6}
\@LN{456}{6}
\@LN{457}{6}
\@LN{458}{6}
\@LN{459}{6}
\@LN{460}{6}
\@LN{461}{6}
\@LN{462}{6}
\@LN{463}{6}
\@LN{464}{6}
\@LN{465}{6}
\bibcite{UriSha}{{34}{2017}{{Shalit et~al.}}{{Shalit, Johansson, and Sontag}}}
\bibcite{vowels2021targeted}{{35}{2021}{{Vowels et~al.}}{{Vowels, Camgoz, and Bowden}}}
\bibcite{Anpeng}{{36}{2023}{{Wu et~al.}}{{Wu, Yuan, Kuang, Li, Wu, Zhu, Zhuang, and Wu}}}
\bibcite{SITE}{{37}{2018}{{Yao et~al.}}{{Yao, Li, Li, Huai, and Zhang}}}
\bibcite{ganite}{{38}{2018}{{Yoon et~al.}}{{Yoon, Jordon, and Van Der~Schaar}}}
\bibcite{TEDEV}{{39}{2021}{{Zhang et~al.}}{{Zhang, Liu, and Li}}}
\@LN@col{1}
\@LN{466}{7}
\@LN{467}{7}
\@LN{468}{7}
\@LN{469}{7}
\@LN{470}{7}
\@LN{471}{7}
\@LN{472}{7}
\@LN{473}{7}
\@LN{474}{7}
\@LN{475}{7}
\@LN{476}{7}
\@LN{477}{7}
\@LN{478}{7}
\@LN{479}{7}
\@LN{480}{7}
\@LN{481}{7}
\@LN{482}{7}
\@LN{483}{7}
\@LN{484}{7}
\@LN{485}{7}
\@LN@col{2}
\gdef \@abspage@last{8}
